#!/usr/bin/env python3

import polars as pl
import argparse
from pathlib import Path

def getCommandLineArguments():
    parser = argparse.ArgumentParser(description="""This python script reads the list of allocations generated by mosalloc,
along with the pebs analysis generated by mosmodel, and creates a ranked list of allocations based on how many TLB misses occured in each one
assuming they do not overlap""")
    parser.add_argument('-a', '--allocation_data', type=Path, required=True,
            help='the path to the log file containing a list of allocations and their stack-trace')
    parser.add_argument('-p', '--pebs_data', type=Path, required=True,
            help='the CSV generated from pebs using mosmodel, for the number of TLB misses per 2MB region')
    parser.add_argument('-b', '--base_data', type=Path, required=True,
            help='the path to the CSV containing the base virtual addresses used the memory allocations')
    parser.add_argument('-o', '--output_file', type=Path, required=True, help='the path to the file where the output will be written')
    args = parser.parse_args()
    return args

# read the list of allocations and create a dataframe, with an entry per allocation
# containing the start of the allocation, its end, and the context (stack-trace) of the malloc call
def read_allocations(allocs: Path) -> pl.DataFrame:
    with allocs.open() as a:
        # read each allocation in stages
        # as each line can be either the start, the end or part of the trace
        # and each need a different rteatment
        stage = 0
        start_list = []
        end_list = []
        ctx_list = []
        allocations_dict = {}
        for line in a.readlines():
            line = line.strip()
            if line == '':
                stage = 0
                ctx_list.append("")
                continue
            if stage == 0:
                start_list.append(int(line, base=16))
                stage = 1
            elif stage == 1:
                end_list.append(int(line, base=16))
                stage = 2
            else:
                ctx_list[-1] += ":" if ctx_list[-1] != "" else ""
                # remove the virtual memory address from the backtrace
                ctx_list[-1] += line[0:line.rindex('[')]

        return pl.DataFrame({"start": start_list, "end": end_list, "context": ctx_list})

# computes a data frame where each entry is the context and how many TLB misses happened,
# on huge page regions overlapping it, in decending order
def rank_allocations(allocs: Path, pebs: Path, base: Path) -> pl.DataFrame:
    pebs_df = pl.read_csv(pebs).lazy()
    # use only pebs on allocations made with brk
    brk_pebs_df = pebs_df.filter(pl.col("PAGE_TYPE") == "brk")

    base_df = pl.read_csv(base)

    # get the start of the brk memory pool of the last entry (which is the application) parse and find its corresponding huge page number
    pool_base = base_df[-1].select(pl.col("brk-start").apply(lambda x: int(x, base=16) // (1 << 21)))

    # add the base page number to the entries, so that the pages start from address 0
    pebs_normalized_df = brk_pebs_df.with_columns(pl.col("PAGE_NUMBER") + pool_base)

    allocs_df = read_allocations(allocs).lazy()

    # convert allocs to to using page numbers instead of addresses
    allocs_pages_df = allocs_df.with_columns(pl.col("start") // (1 << 21), pl.col("end") // (1 << 21))

    # match a pebs entry with the allocation overlapping with its page
    # by first creating a cross product of the two, filtering by whether the page is containing in the pages containing the allocation
    allocs_with_misses_df = allocs_pages_df.join(pebs_normalized_df, on=1).filter((pl.col("start") <= pl.col("PAGE_NUMBER")) & (pl.col("end") >= pl.col("PAGE_NUMBER")))
    # find the allocation contexts that cause the most tlb misses for the least number of memory usage (so that we can choose the handlful that are most effective)
    return allocs_with_misses_df.collect() \
                                .groupby("context") \
                                .agg(pl.col("NUM_ACCESSES").sum(), memory_usage=(pl.col("end") - pl.col("start")).sum()) \
                                .sort(pl.col("NUM_ACCESSES") / pl.col("memory_usage"), descending=True)

if __name__ == "__main__":
    args = getCommandLineArguments()

    ranked_allocations = rank_allocations(args.allocation_data, args.pebs_data, args.base_data)

    ranked_allocations.write_csv(args.output_file)
