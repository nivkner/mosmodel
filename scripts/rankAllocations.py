#!/usr/bin/env python3

import polars as pl
import argparse
from pathlib import Path

def getCommandLineArguments():
    parser = argparse.ArgumentParser(description="""This python script reads the list of allocations generated by mosalloc,
along with the pebs analysis generated by mosmodel, and creates a ranked list of allocations based on how many TLB misses occured in each one
assuming they do not overlap""")
    parser.add_argument('-a', '--allocation_data', type=Path, required=True,
            help='the path to the log file containing a list of allocations and their stack-trace')
    parser.add_argument('-p', '--pebs_data', type=Path, required=True,
            help='the CSV generated from pebs using mosmodel, for the number of TLB misses per 2MB region')
    parser.add_argument('-b', '--base_data', type=Path, required=True,
            help='the path to the CSV containing the base virtual addresses used the memory allocations')
    parser.add_argument('-B', '--budget', type=int,
            help='the maximum number of huge pages that should be allocated')
    parser.add_argument('-o', '--output_file', type=Path, required=True, help='the path to the file where the output will be written')
    args = parser.parse_args()
    return args

# computes a data frame where each entry is the context and how many TLB misses happened,
# on huge page regions overlapping it, in decending order
def rank_allocations(allocs: Path, pebs: Path, base: Path) -> pl.DataFrame:
    pebs_df = pl.scan_csv(pebs)
    # use only pebs on allocations made with brk
    brk_pebs_df = pebs_df.filter(pl.col("PAGE_TYPE") == "brk")

    base_df = pl.read_csv(base)

    # get the start of the brk memory pool of the last entry (which is the application) parse and find its corresponding huge page number
    pool_base = base_df[-1].select(pl.col("brk-start").apply(lambda x: int(x, base=16) // (1 << 21)))

    # add the base page number to the entries, so that the pages start from address 0
    pebs_normalized_df = brk_pebs_df.with_columns(pl.col("PAGE_NUMBER") + pool_base)

    allocs_df = pl.scan_csv(allocs)

    # convert allocs to to using page numbers instead of addresses
    allocs_pages_df = allocs_df.with_columns(pl.col("start") // (1 << 21), pl.col("end") // (1 << 21))

    # match a pebs entry with the allocation overlapping with its page
    # by first creating a cross product of the two, filtering by whether the page is containing in the pages containing the allocation
    allocs_with_misses_df = allocs_pages_df.join(pebs_normalized_df, how="cross").filter((pl.col("start") <= pl.col("PAGE_NUMBER")) & (pl.col("PAGE_NUMBER") <= pl.col("end")))
    # find the allocation contexts that cause the most tlb misses for the least number of memory usage (so that we can choose the handlful that are most effective)
    return allocs_with_misses_df.collect() \
                                .groupby("context") \
                                .agg(pl.col("NUM_ACCESSES").sum(), memory_usage=(pl.col("end") - pl.col("start")).sum()) \
                                .sort(pl.col("NUM_ACCESSES") / pl.col("memory_usage"), descending=True)

if __name__ == "__main__":
    args = getCommandLineArguments()

    ranked_allocations = rank_allocations(args.allocation_data, args.pebs_data, args.base_data)

    if args.budget is not None:
        ranked_allocations = ranked_allocations.filter(pl.col("memory_usage").cumsum() <= args.budget)

    ranked_allocations.write_csv(args.output_file)
